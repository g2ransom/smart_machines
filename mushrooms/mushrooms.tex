\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator*{\argmin}{argmin}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\setlength{\parindent}{0pt}
\title{Deadly Mushroom Classification}
\author{Garrett Ransom}
\date{October 2018}

\begin{document}
	\maketitle
	\begin{abstract}
		\noindent
		Eating certain types of mushrooms can be the difference between life and death. In this paper, we evaluate the performance of various classification models that can assist us in distinguishing poisonous mushrooms from edible ones. Our dataset is composed of 8124 examples described by 22 categorical variables and one binary predictor variable.
	\end{abstract}

\section{Introduction}
In this report, we give a brief theoretical introduction to the algorithms that we'll be using for our machine learning task. Then, we give detail into our expirement results, followed by a brief conclusion. The goal of this paper is to find the best algorithm(s) for the task of predicting whether or not a mushroom is poisonous by a set of categorical features.


\section{Classification Models}

\subsection{Logistic Regression}
In the case of Logistic Regression, we use the sigmoid function as a means to map our predictor variable as a probabilty such that $y \in \{0, 1\}$. 

$$h_\theta(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}}$$ where
$$g(z) = \frac{1}{1 + e^{-z}}$$

We maximize the log likelihood function to find the best fit for $\theta$.

$$\ell(\theta) = \sum_{i=1}^{m} y^{(i)}\log h(x^{(i)}) + (1-y^{(i)})\log(1 - h(x^{(i)})) $$

We maximize the log likelihood by using gradient descent given by $\theta := \theta - \alpha\nabla_\theta \ell(\theta)$. By taking the partial derivatives with respect to $\theta$ for one training example $(x,y)$, we get the following equation:

$$\frac{\partial}{\partial \theta_j} \ell(\theta) = (y - h_\theta(x))x_j$$

Therefore, using stochastic gradient descent rule gives us:

$$ \theta_j := \theta_j - \alpha(y^{(i)} - h_\theta(x^{(i)} )) x^{(i)}_j $$

\subsection{Support Vector Machines}

In the case of Support Vector Machines, we create a classifier by using the equation
$$h_{w,b} = g(w^{T}x + b).$$

Given a training example $(x^{(i)}, y^{(i)})$, we can form the idea of a functional margin of $(w, b)$ with respect to the training example as
$$ \hat{\gamma}^{(i)} = y^{(i)}(w^{T}x + b).$$

Given a training set $S = \{(x^{(i)}, y^{(i)}); i=1,...,m \}$, we can define the functional and geometric margin as the smallest margin of across all training examples

$$\gamma = \min_{i=1,...,m} \hat{\gamma}^{(i)}.$$

Ultimately, by doing some cool math, we can use the following problem for finding the optimal margin classifier

\begin{flalign}
&\min_{w,b}  \frac{1}{2}||w||^2 \\
&\text{s.t} \; \; \;  y^{(i)}(w^{T}x + b) \geq 1, i = 1,...,m
\end{flalign}

\subsection{Decision Trees}
Suppose we want to create a decision tree for a classification task. If we have $N$ training examples composed of $p$ inputs $(x_i, y_i)$ for $i,...,N$ with $x_i = (x_{i1}, x_{i2},...,x_{ip})$, then there exists a set of splitting points that best classify output variables. We can split the data into $M$ regions $R_1, R_2,...,R_M$ and we can model the response as a constant $c_m$ in every region:

$$f(x) = \sum_{m=1}^{M} c_m I(x \in R_m).$$

We can use the Gini Index, Misclassification Error, or Cross-Entropy to find the best splitting points.
$$\text{Gini Index:} \sum_{k \neq k'} \hat{p}_{mk} \hat{p}_{mk'} = \sum_{k=1}^{K} \hat{p}_{mk}(1 - \hat{p}_{mk})$$
\subsection{Random Forests}
Random forests use bootstrapping or bagging during training. The core purpose of random forests is to correct decision trees' natural tendency towards overfitting training data.

\noindent\rule{13cm}{1.9pt} \\
 \textbf{Algorithm} \textit{Random Forest Classifier}  \\
 \noindent\rule{13cm}{0.8pt} \\
1. For $b=1$ to $B$: \\
\hspace*{10mm} (a) Draw a bootstrap sample of size $N$ from $X$, $Y$ and call these $X_b$, \hspace*{18mm}$Y_B$. \\
\hspace*{10mm} (b) Train a classification free $f_b$ on $X_b$, $Y_b$. \\

Let $\hat{C}_{b}(x) $ be the classification predication for the $b$th random forest tree. The output is the class that wins majority vote. \\
\noindent\rule{13cm}{1.9pt} \\  

\subsection{Naive Bayes}

The Naive Bayes classifier is a model based on Bayes' theorem with the assumption of independence between features. Bayes' Rule tells us that
$$p(A | B) = \frac{p(A)p(B | A) }{p(B)}.$$
$p(A|B)$ is considered the posterior, while $p(A)$ is the prior andtio $p(B | A)$ is the likelihood and $p(B)$ is the evidence. \\

But how does this apply to classification? If we represent a vector $\vec{x} = (x_1,...,x_n)$ by $n$ independent features, we calculate the probability of $\vec{x}$ belonging to a class $C_k$ as $p(C_k | x_1,...,x_n)$. Referring back to Bayes' theorem, the conditional probability can be represented as

$$p(C_k | \vec{x}) = \frac{p(C_k)p(\vec{x} | C_k)}{p(\vec{x})}.$$

In practice, we only pay attention to the numerator of this equation. We can further break down the representation into the joint probability model

$$p(C_k, x_1,...,x_n)$$

which can be further broken down by using the chain rule of probability


$$p(C_k, x_1,..., x_n) = p(x_1 | x_2,...,x_n, C_k)p(x_2 | x_3,...,x_n, C_k)...p(x_{n-1} | x_n, C_k)p(x_n | C_k)p(C_k).$$ 

Now, conditional independence comes into play, where we can assume that for each feature $x_i$, every other feature $x_j$ for $j \neq i$ is conditionally independent relative to $x_i$. This means that 
$$p(x_i | x_i+1,...,x_n, C_k) = p(x_i | C_k).$$

Therefore, the joint model can be represented as $$p(C_k | x_1,...,x_n) = p(C_k) \prod_{i=1}^{n}p(x_i | C_k).$$

\section{Experiment Results and Conclusion}
\begin{center}
\begin{tabular}{ |c|c|}
\hline
Algorithm & Accuracy Score \\
\hline
Naive Bayes & 0.9327 \\
Logisitc Regression & 1.0 \\
Support Vector Machine & 0.997 \\
Decision Tree & 1.0 \\
Random Forest & 1.0 \\
\hline
\end{tabular}
\end{center}

Here, we must take note that even though all algorithms provide accuracy scores over 90 percent, that our task requires near perfection. Misclassifying a poisonous mushroom is the difference between life and death. Because of this, Naive Bayes actually provides an extremely low score by our criteria. We see that Logistic Regression, Decision Trees, and Random Forests all provide an accuracy score of 100 percent. Therefore, all three of them are quality choices for classifying mushrooms in the future.


\end{document}